<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Project Atomic</title>
  <subtitle>Tag: Kubernetes</subtitle>
  <id>http://www.projectatomic.io/blog/</id>
  <link href="http://www.projectatomic.io/blog/"/>
  <link href="http://www.projectatomic.io/blog/tag/kubernetes.xml" rel="self"/>
  <updated>2019-11-21T00:00:00+00:00</updated>
  <author>
    <name>Project Atomic. Sponsored by Red Hat, Inc.</name>
  </author>
  <entry>
    <title>Building a Buildah container image for Kubernetes</title>
    <link rel="alternate" href="http://www.projectatomic.io/blog/2018/03/building-buildah-container-image-for-kubernetes/"/>
    <id>http://www.projectatomic.io/blog/2018/03/building-buildah-container-image-for-kubernetes/</id>
    <published>2018-03-01T00:00:00+00:00</published>
    <updated>2021-10-07T14:03:16+00:00</updated>
    <author>
      <name>ipbabble</name>
    </author>
    <content type="html">&lt;p&gt;&lt;img alt="buildah logo" src="https://cdn.rawgit.com/projectatomic/buildah/master/logos/buildah-logo_large.png" /&gt;&lt;/p&gt;

&lt;h3&gt;Building a Buildah Container Image for Kubernetes&lt;/h3&gt;

&lt;h4&gt;Background&lt;/h4&gt;

&lt;p&gt;Dan Walsh (&lt;a href="https://github.com/rhatdan"&gt;@rhatdan&lt;/a&gt;) asked me to look into building a working &lt;a href="https://github.com/projectatomic/buildah"&gt;Buildah&lt;/a&gt; container image. This was not just a cool experiment. It has a real purpose. As many readers know, Dan is not a fan of &lt;a href="https://www.youtube.com/watch?v=BeRr3aZbzqo&amp;amp;list=PLaR6Rq6Z4IqfhzC5ds3sMju7KKNzdd0xy&amp;amp;t=1055s"&gt;&lt;q&gt;big fat daemons&lt;/q&gt;&lt;/a&gt;. This has become less of an issue when running containers in Kubernetes as there is an alternative with &lt;a href="http://cri-o.io/"&gt;CRI-O&lt;/a&gt;. CRI-O provides kubernetes a standard interface to &lt;a href="https://github.com/opencontainers/runtime-spec"&gt;OCI compliant runtimes&lt;/a&gt;. &lt;a href="https://github.com/opencontainers/runc"&gt;runC&lt;/a&gt; is the reference implementation of the OCI runtime specification. Kubernetes calls the runtC runtime through CRI-O and runC then talks to the Linux kernel to run a container. This bypasses the need for the Docker daemon, and containerd. With CRI-O, there is no requirement for the Docker daemon for a kubernetes cluster to run containers.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;However this does not solve the problem of building container images. Until recently, Docker was considered the gold standard for building OCI compliant container images - and with Docker, a running daemon is required to build them. There are two ways to solve this: have dedicated build nodes or run the Docker daemon across the cluster, which puts us back at square zero. &lt;/p&gt;

&lt;p&gt;The daemon runs as root, and adds complexity and attack surface. To mitigate this risk, having dedicated machines for doing builds seems the better choice. But when you have a cluster of resources with something like Kubernetes you really don’t want to waste resources with dedicated nodes which might sit idle when not doing builds. It’s much better to schedule builds in the cluster, just like any other process. There are several reasons for this.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In a continuous integration, multi-tenant environment there can be multiple builds going on at any time. So if your cluster is a PaaS for developers you want to be able to schedule builds whenever the developer needs them as quickly as possible. Having the ability to schedule across the cluster is very efficient.&lt;/li&gt;
&lt;li&gt;When new base images become available in a continuous deployment environment, you will want to take advantage of them as soon as possible. This may cause a spike of build activity that you want to spread across the cluster rather than overloading a single machine.&lt;/li&gt;
&lt;li&gt;Related to the second point, when security events like a CVE occurs, many images will need to be rebuilt to ensure the vulnerability is addressed. Again this is going to cause spikes and will require many simultaneous build resources. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore it is important to be able to be able to schedule container image builds within the kubernetes cluster. But it&amp;rsquo;s hardly worth having solved the &lt;q&gt;big fat daemon&lt;/q&gt; issue for runtime if you still need the daemon for build time across the cluster. i.e. you still need to have Docker running on all the nodes in the cluster if you intend to do builds on them.&lt;/p&gt;

&lt;p&gt;Enter Buildah. Buildah doesn’t require a daemon to run on a host in order to build a container image. At the same time it would be great if we didn’t have to install Buildah on every node in the cluster, as we did with Docker, and also maintain consistent updates on each node. Instead it would be preferable to run Buildah as a container. If we can.&lt;/p&gt;

&lt;p&gt;So I embarked on this effort and hit several small roadblocks, but essentially got it working relatively quickly.&lt;/p&gt;

&lt;h4&gt;Building a Buildah OCI &lt;a href="https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/#h.dqlu6589ootw"&gt;image&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;First let’s summarize the goal and purpose of this exercise. We want to build a Buildah container that can be run by Kubernetes to perform image builds across the cluster on demand. This allows kubernetes to orchestrate build farms. If we can do this then we can remove the need for running a Docker daemon everywhere on the Kubernetes cluster. CRI-O and runC solve the runtime problem and CRI-O, runC and Buildah solve the build problem.&lt;/p&gt;

&lt;p&gt;Time to install Buildah. Run as root because you will need to be root for running Buildah commands for this exercise. My Linux of choice is Fedora and so I use DNF.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# dnf -y install buildah
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here are the steps I performed to build my Buildah OCI image:&lt;/p&gt;

&lt;p&gt;First I wanted to build it from scratch. I wanted nothing in the container except what I need to run Buildah. But what about DNF or Yum, you might ask? Don&amp;rsquo;t need them. If I&amp;rsquo;m using this Buildah container from the command line then I&amp;rsquo;ll be using the host’s package manager, whatever that might be. If I&amp;rsquo;m using Buildah &lt;code&gt;bud&lt;/code&gt;, aka build-using-dockerfile, then I&amp;rsquo;m using the FROM images package manager. To start the process, I created a scratch container and stored the container&amp;rsquo;s name, which happens to be working-container, in a environment variable for convenience.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# containerid=$(buildah from scratch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I need to mount the container&amp;rsquo;s file system so that I can install the buildah package and dependencies.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# scratchmnt=$(buildah mount $containerid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next I install the buildah package and dependencies into the containers filesystem. Notice that as I&amp;rsquo;m doing this from a Fedora laptop I&amp;rsquo;m defaulting to the Fedora repositories and I&amp;rsquo;m specifying the version, 27. Also, I clean up afterward so that we can reduce the image size. If you skip the &lt;code&gt;dnf clean&lt;/code&gt; step you&amp;rsquo;ll have extra bloat.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# dnf install --installroot $scratchmnt --release 27 buildah --setopt install_weak_deps=false -y
# dnf clean all --installroot $scratchmnt --release 27
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to set the container ENTRYPOINT and CMD. It&amp;rsquo;s good practice to use ENTRYPOINT for the command that you want to run. When containerizing a command line tool, I usually set CMD to &lt;code&gt;--help&lt;/code&gt; so that it gets appended to the ENTRYPOINT if you don&amp;rsquo;t specify any parameters. I got into the practice after reading Michael Crosby’s &lt;a href="http://crosbymichael.com/dockerfile-best-practices.html"&gt;Best Practices&lt;/a&gt; post. See section 5. &lt;/p&gt;

&lt;p&gt;(Currently Buildah needs ‘\’ for parameters with ‘&amp;ndash;’. That will get fixed.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# buildah config --cmd "\-\-help" $containerid
# buildah config --entrypoint /usr/bin/buildah $containerid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It’s also good practice, and pretty important, to set some other metadata like the image name and who created it etc.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# buildah config --author "wgh @ redhat.com" --created-by "ipbabble" --label name=buildah $containerid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally I need to commit the container to an image. It should be called &lt;code&gt;buildah&lt;/code&gt; because it is good practice to name a command line tool container the name of the command. That way, if its ENTRYPOINT is set correctly, you can run it similar to the command it is containerizing. The following command ran a little over a minute and currently has no status output. So be patient.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# buildah commit $containerid buildah
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A bonus step is to push it to a registry so that other people can use it. I used the Podman client to do that. Below I push to the &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt; public registry. In order to do that you need to create an account. Click on the “Create Account” link on the &lt;a href="https://quay.io/signin/"&gt;sign in page&lt;/a&gt;. Push to Quay just like Docker. (Quay supports OCI version 1.0 or above). You can install Podman (a daemon-less alternative to Docker) using DNF and then use it to authenticate with and then push the image to quay.io.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# dnf -y install podman
# podman login quay.io
Username: ipbabble
Password:
Login Succeeded!
# podman push buildah quay.io/ipbabble/buildah
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Running the Buildah container using Podman&lt;/h4&gt;

&lt;p&gt;To test it you can also use the Podman client. That way you don&amp;rsquo;t have to install Docker and the &lt;q&gt;big fat daemon&lt;/q&gt;. In order to do this there are a number of parameters required. Of course the purpose of this exercise to build a Buildah container for use with Kubernetes. So this will be run by runC through CRI-O and not Podman.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For the main use case you need to run Podman as a privileged container. This will change in the future, but for right now building a container image requires access to the underlying &lt;code&gt;/var/lib/containers&lt;/code&gt; directory so the the image is persisted and can be shared with others on the host. But for this case we&amp;rsquo;ll make use of the &lt;code&gt;--privileged&lt;/code&gt; parameter in &lt;code&gt;podman run&lt;/code&gt;. It doesn’t require &lt;code&gt;/var/lib/containers&lt;/code&gt; for building, just for sharing. You could bind mount another directory that doesn’t need privileges. See more about that below in “The case for no privilege”. There are some considerations.&lt;/li&gt;
&lt;li&gt;You probably need network access if you are going to run something like &lt;code&gt;buildah bud&lt;/code&gt;. You don&amp;rsquo;t need this if you are just running &lt;code&gt;buildah images/containers&lt;/code&gt; etc. But many use cases will be using &lt;code&gt;bud&lt;/code&gt; and so I&amp;rsquo;ve just used the hosts network and now the bridge because it&amp;rsquo;s a short running build process. So for the example I&amp;rsquo;ll use the &lt;code&gt;--network host&lt;/code&gt; parameter for &lt;code&gt;podman run&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;You&amp;rsquo;ll need to bind mount an area of the file system, so that &lt;code&gt;buildah bud&lt;/code&gt; can build up and then commit the image to the host file system. This can be &lt;code&gt;/var/lib/containers&lt;/code&gt; if you like, so as it’s immediately shared shared on the host, or it can be some build sandbox area etc. You also need to bind mount the Dockerfile path so that Buildah can see the Dockerfile. So for the example I’ll use the bind mount parameter &lt;code&gt;-v&lt;/code&gt; or &lt;code&gt;--volume&lt;/code&gt; parameter for &lt;code&gt;podman run&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Use your favorite Nginx Dockerfile or the one below. &lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# Base on the Fedora
FROM fedora:latest
MAINTAINER http://fedoraproject.org/wiki/Cloud

RUN dnf -y update &amp;amp;&amp;amp; dnf clean all
RUN dnf -y install nginx &amp;amp;&amp;amp; dnf clean all
RUN echo "daemon off;" &amp;gt;&amp;gt; /etc/nginx/nginx.conf
RUN echo "nginx on Fedora" &amp;gt; /usr/share/nginx/html/index.html

EXPOSE 80

ENTRYPOINT [ "/usr/sbin/nginx" ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command builds an Nginx container image called ‘nginx’. Obviously using Podman (or Docker) is overkill because they both have build capabilities. Ideally this container would be run in a pod on kubernetes through CRI-O. My Dockerfile is in my &lt;code&gt;/home/whenry/dfs&lt;/code&gt; directory (shorter for dockerfiles). So I bind mount that directory to &lt;code&gt;/tmp&lt;/code&gt; inside the container. Inside it’s &lt;code&gt;/tmp/Dockerfile&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# podman run --privileged --network host -v /var/lib/containers:/var/lib/containers:rw  -v /home/whenry/dfs:/tmp:Z buildah bud -f /tmp/Dockerfile -t nginx 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Currently, due to OverlayFS requiring root privileges, this container must run in privileged mode. This requirement should be deprecated when Fedora’s OverlayFS does not require a privileged user.&lt;/p&gt;

&lt;h4&gt;Current issues with non privileged mode&lt;/h4&gt;

&lt;p&gt;There is a requirement in large clusters for buildah not to run in privileged mode. There are ways to mitigate this but there will be cases where people demand it.&lt;/p&gt;

&lt;p&gt;I had prepared an example of how to do this but I will keep it for a later blog.  Here are some of the current constraints I encountered trying to solve the non-privileged problem.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you want to share the builds immediately through the hosts &lt;code&gt;/var/lib/containers&lt;/code&gt; then you need to run Buildah as privileged. You could build somewhere else and then move the images to &lt;code&gt;/var/lib/containers&lt;/code&gt; - see below. &lt;/li&gt;
&lt;li&gt;If you don&amp;rsquo;t care to share on the host immediately then you can use unprivileged (don’t use &amp;ndash;privileged) but you need to use a different directory than /var/lib/containers. However currently OverlayFS requires privileged mode on Fedora/RHEL. This should be fixed soon. You could use &lt;code&gt;--storage-driver=vfs&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you decide to build inside the container you can do that with unprivileged, but you hit the overlay on overlay issue and will then need to run with &lt;code&gt;--storage-driver vfs&lt;/code&gt;. This worked at one point but I’ve seen a regression. I am investigating.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Watch for a progress upstream and for a new blog post on using Buildah unprivileged in a later blog. &lt;/p&gt;

&lt;h4&gt;Final thanks and some helpful links&lt;/h4&gt;

&lt;p&gt;Feel free to pull the Buildah image  ipbabble/buildah from Quay.io. Have fun. Thanks to &lt;a href="https://github.com/nalind"&gt;@nalind&lt;/a&gt;, &lt;a href="https://github.com/nalind"&gt;@tsweeney&lt;/a&gt; (so many great edits) , &lt;a href="https://github.com/fatherlinux"&gt;@fatherlinux&lt;/a&gt;,  &lt;a href="https://twitter.com/bbaude"&gt;@bbaude&lt;/a&gt;, &lt;a href="https://github.com/rhatdan"&gt;@rhatdan&lt;/a&gt;, &lt;a href="https://twitter.com/rossturk"&gt;@rossturk&lt;/a&gt;, and &lt;a href="https://github.com/bparees"&gt;@bparees&lt;/a&gt; (confirming the kubernetes use case) for all the input along the way. &lt;/p&gt;

&lt;p&gt;If you have any suggestions or issues please post them at the &lt;a href="https://github.com/projectatomic/buildah/issues"&gt;Project Atomic Buildah Issues&lt;/a&gt; page. For more information on Buildah and how you might contribute please visit the &lt;a href="https://github.com/projectatomic/buildah"&gt;Buildah home page&lt;/a&gt; on Github including &lt;a href="https://github.com/projectatomic/buildah/blob/master/docs/tutorials/README.md"&gt;tutorials&lt;/a&gt;.  For more information on the buildah system container see &lt;a href="https://github.com/projectatomic/atomic-system-containers/buildah-fedora/config.json.template"&gt;here&lt;/a&gt;. My previous blogs on Buildah: &lt;a href="http://www.projectatomic.io/blog/2017/11/getting-started-with-buildah/"&gt;Intro to Buildah&lt;/a&gt;, &lt;a href="http://www.projectatomic.io/blog/2018/01/using-image-registries-with-buildah/"&gt;Using Buildah with registries&lt;/a&gt;. Information on Podman can be found &lt;a href="https://github.com/projectatomic/libpod"&gt;here&lt;/a&gt;.  Podman man pages &lt;a href="https://github.com/projectatomic/libpod/tree/master/docs"&gt;here&lt;/a&gt;. &lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Reintroduction of Podman</title>
    <link rel="alternate" href="http://www.projectatomic.io/blog/2018/02/reintroduction-podman/"/>
    <id>http://www.projectatomic.io/blog/2018/02/reintroduction-podman/</id>
    <published>2018-02-26T00:00:00+00:00</published>
    <updated>2021-10-07T14:03:16+00:00</updated>
    <author>
      <name>Dan Walsh</name>
    </author>
    <content type="html">&lt;p&gt;Podman (formerly kpod) has been kicking around since last summer.  It was originally part of the &lt;a href="https://github.com/kubernetes-incubator/cri-o&amp;amp;sa=D&amp;amp;ust=1519653090854000&amp;amp;usg=AFQjCNGVeTeYAfYk3RH27hK5ykSNrATy1w"&gt;CRI-O&lt;/a&gt; project.  We moved podman into a separate project, &lt;a href="https://github.com/projectatomic/libpod"&gt;libpod&lt;/a&gt;.  We wanted Podman and CRI-O to develop at their own pace.  Both CRI-O and Podman work fine as independent tools and also work well together.&lt;/p&gt;

&lt;p&gt;The goal of Podman (Pod Manager) is to offer an experience similar to the docker command line - to allow users to run standalone (non-orchestrated) containers.  Podman also allows users to run groups of containers called pods. For those that don’t know, a Pod is a term developed for the Kubernetes Project which describes an object that has one or more containerized processes sharing multiple namespaces (Network, IPC and optionally PID).&lt;/p&gt;

&lt;p&gt;Podman brings innovation to container tools in the spirit of Unix commands which do “one thing” well. Podman doesn’t require a daemon to run containers and pods. This makes it a great asset for your container tools arsenal.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h4&gt;Debug, Diagnose, and Manage content for CRI-O&lt;/h4&gt;

&lt;p&gt;One of our goals with Podman is to allow users of CRI-O to examine what is going on behind the scenes in the OpenShift/Kubernetes environment when running CRI-O as the container runtime.  Users of Kubernetes with a Docker Engine backend could use the Docker CLI to examine the containers/images that were created and running in the environment.  If you swap out the Docker Engine with CRI-O then you could examine the containers/images in your environment with Podman.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We have not fully integrated podman and CRI-O at this point but continue to work on the integration.&lt;/p&gt;

&lt;h4&gt;Familiar with the Docker CLI? Don’t Worry!&lt;/h4&gt;

&lt;p&gt;When we started working on the Podman CLI, we wanted to make sure it was easy for people to use, especially for those transitioning from the Docker CLI.  For running containers and managing container images, we followed the same pattern on most of commands and options from the Docker CLI.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;docker run -ti -v /var/lib/myapp:/var/lib/myapp:Z --security-opt seccomp:/tmp/secomp.json fedora sh

podman run -ti -v /var/lib/myapp:/var/lib/myapp:Z --security-opt seccomp:/tmp/secomp.json fedora sh

docker ps -a -q

podman ps -a -q

docker images --format "table {{.ID}} {{.Repository}} {{.Tag}}"

podman images --format "table {{.ID}} {{.Repository}} {{.Tag}}"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You get the idea.  The goal is, if you find a Docker example via your favorite search engine, you will only need to change the docker command to podman.&lt;/p&gt;

&lt;p&gt;Podman implements 38 of the 40 Docker CLI commands defined in Docker 1.13, but there are a few that we don’t implement. For example those dealing with Docker Swarm - instead for orchestrated Pods/Containers we suggest the use of Kubernetes.  Also, some of the commands that deal with Docker Plugins like volume plugins and network plugins are not implemented.   A full breakdown of the Podman commands and their Docker equivalents can be found on the &lt;a href="https://github.com/projectatomic/libpod/blob/master/transfer.md"&gt;&lt;q&gt;Podman Usage Transfer&lt;/q&gt;&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;Podman is implemented as a standalone command. Unlike the Docker CLI which talks to the Docker daemon when examining the content, Podman requires no such daemon to get work done. Podman can examine registry server content directly without any daemon involved.&lt;/p&gt;

&lt;p&gt;This gives Podman a big advantage in many operations scenarios. For example, imagine that the docker daemon hangs in a Kubernetes environment - you can’t inspect images nor inspect which containers are running.  No Docker daemon, no diagnosis.  With podman, you can because it doesn&amp;rsquo;t rely on a daemon.&lt;/p&gt;

&lt;h3&gt;libpod&lt;/h3&gt;

&lt;p&gt;You might have noticed that Podman is not in a github repo called podman, but is actually stored under &lt;a href="https://github.com/projectatomic/libpod"&gt;libpod&lt;/a&gt;. Libpod is a library that we are excited about that actually allows other tools to manage pods/containers.  Podman is the default CLI tool for using this library.  We plan on now porting CRI-O to use the libpod library.  Potentially other tools like &lt;a href="https://github.com/projectatomic/buildah"&gt;Buildah&lt;/a&gt; will also be ported to use libpod as well.  We hope that others projects find some of its features useful, and could potentially use it when trying to implement the Kubernetes Pod concept, but do not need Kubernetes orchestration.&lt;/p&gt;

&lt;h4&gt;No Big Fat Daemons&lt;/h4&gt;

&lt;p&gt;Podman does not implement a daemon like the Docker Engine.&lt;/p&gt;

&lt;p&gt;The Docker CLI is a client/server operation and the Docker CLI communicates with the Docker engine when it wants to create a container.  Most users do not run into this, but it can lead to some issues when using Docker in practice.  You need to start the Docker Daemon before you can use the Docker CLI. When you create a container using Docker, the Docker CLI sends an API call to the Docker Engine to launch the OCI Container runtime, usually runc, to launch the container.  This means the container processes are not &lt;a href="https://www.thegeekstuff.com/2013/07/linux-process-life-cycle/"&gt;descendants&lt;/a&gt; of the Docker CLI, they are descendants of the Docker Engine.&lt;/p&gt;

&lt;p&gt;Podman on the other hand does not use a daemon.  It also uses OCI runtimes, usually runc, to launch the container, but the container processes are direct &lt;a href="https://www.thegeekstuff.com/2013/07/linux-process-life-cycle/"&gt;descendants&lt;/a&gt; of the podman process.  Podman is more of a traditional fork/exec model of Unix and Linux.&lt;/p&gt;

&lt;p&gt;The advantage of the Podman model is that cgroups or security constraints still control the container processes.  If I constrain the podman command with cgroups the container processes will receive those same constraints.  Another feature of this model is that I can take advantage of some of the advanced features of systemd by putting podman into a systemd unit file.&lt;/p&gt;

&lt;p&gt;For example, systemd has a concept of a notify unit file.  When booting a system you might want to run a service in a container that has to be started before other services that require that service get started.  A notify unit waits until the primary service sends a notify signal to systemd, saying it is up and running.  With the Docker CLI there is no way to implement this, since it is a client/server operation.  Podman just forwards the systemd information down to its children, so that the container process can notify systemd when it is ready to receive connections.  Podman can also support systemd socket activation.&lt;/p&gt;

&lt;h4&gt;But wait there&amp;rsquo;s more &amp;hellip;&lt;/h4&gt;

&lt;p&gt;While we started with the Docker CLI, we by no means want to stop there.  We have added additional features to traditional commands that our engineering team finds convenient and always aggravated us when using the Docker CLI, and &lt;a href="https://github.com/moby/moby/issues/1682"&gt;were never able to get merged into upstream Docker/CLI&lt;/a&gt;.  For example we have added a &lt;code&gt;--all&lt;/code&gt; flag to several commands.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;podman rm --all

podman rmi --all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two commands remove all containers and all images from your system.&lt;/p&gt;

&lt;p&gt;We plan to introduce &lt;a href="https://github.com/projectatomic/libpod/issues/341"&gt;pod commands&lt;/a&gt; to make it easier for users to create and manage &lt;q&gt;pods&lt;/q&gt;.&lt;/p&gt;

&lt;h3&gt;containers/storage and containers/image&lt;/h3&gt;

&lt;p&gt;In the spirit of reuse, I wanted to point out the other two libraries that are critical to making Podman possible. &lt;a href="https://github.com/containers/storage"&gt;containers/storage&lt;/a&gt; is the library that allows us to use copy-on-write (COW) file systems, required to run containers.  We share this storage between all of our tools: &lt;a href="https://github.com/kubernetes-incubator/cri-o"&gt;CRI-O&lt;/a&gt;, &lt;a href="https://github.com/projectatomic/buildah"&gt;Buildah&lt;/a&gt;, &lt;a href="https://github.com/projectatomic/skopeo"&gt;Skopeo&lt;/a&gt;, as well as Podman.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/containers/image"&gt;containers/image&lt;/a&gt; is the library that allows us to download and install OCI Based Container Images from containers registries like Docker.io, &lt;a href="https://coreos.com/quay-enterprise/"&gt;Quay&lt;/a&gt;, and Artifactory, as well as many others.  Container/image even allows us to easily move container images in and out of Docker image storage and Podman container/storage as well as container registries.&lt;/p&gt;

&lt;p&gt;But, unlike the Docker storage and image management, containers/storage and containers/image were built from the ground up to support multiple independent processes to interact with the libraries at the same time.  This means you can be running a full Kubernetes environment with CRI-O while at the same time you are building container images using Buildah and managing your containers and pods with Podman.&lt;/p&gt;

&lt;h3&gt;Try it out&lt;/h3&gt;

&lt;p&gt;Podman is now available in Fedora 28 and 27.  It is also available in Ubuntu (via &lt;a href="https://launchpad.net/~projectatomic/+archive/ubuntu/ppa"&gt;Project Atomic PPA&lt;/a&gt; and CentOS (via &lt;a href="https://cbs.centos.org/repos/virt7-container-common-candidate/x86_64/os/"&gt;Virtualization SIG yum repository&lt;/a&gt; and should be available in Red Hat Enterprise Linux 7.5 release.  It should be easy to build and install in your favorite distribution.  This is an Alpha release, and we are anxious to hear your feedback.  We see Podman as a viable alternative to Docker for a lot of your container work loads, without requiring a large learning curve.&lt;/p&gt;

&lt;p&gt;One last note, I pointed out above that podman manages the containers, images and storage directly, this is similar to the Docker CLI, but sometimes (ex. debugging) you still want to communicate with the CRI-O daemon directly, using the same interface that Kubernetes uses, but outside of Kubernetes.  We are also providing a new tool called &lt;a href="https://github.com/kubernetes-incubator/cri-tools"&gt;crictl&lt;/a&gt; that can perform all of the CRI calls that Kubernetes defines. In a future blog, I will explain this further.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Atomic and Friends at KubeCon</title>
    <link rel="alternate" href="http://www.projectatomic.io/blog/2017/11/atomic-at-kubecon/"/>
    <id>http://www.projectatomic.io/blog/2017/11/atomic-at-kubecon/</id>
    <published>2017-11-29T18:00:00+00:00</published>
    <updated>2021-10-07T14:03:16+00:00</updated>
    <author>
      <name>Josh Berkus</name>
    </author>
    <content type="html">&lt;p&gt;Going to KubeCon?  Contributors from Project Atomic, OpenShift Origin, and other related projects will be all over the conference, including talks, salons, and a whole schedule of back-to-back demos in the Red Hat booth. Read on for details on what to see at Kubecon.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The week kicks off with &lt;a href="http://openshiftgathering.com/openshiftgathering/austin"&gt;OpenShift Commons&lt;/a&gt;, including &lt;q&gt;The State of the Container Ecosystem&lt;/q&gt; with Dan Walsh and Mrunal Patel, and &lt;q&gt;OpenShift on AWS&lt;/q&gt; by Fedora Atomic contributor David Duncan.  As of this post, registration is still open.&lt;/p&gt;

&lt;p&gt;Mrunal Patel is then &lt;a href="https://kccncna17.sched.com/event/CU6T/cri-o-all-the-runtime-kubernetes-needs-and-nothing-more-mrunal-patel-red-hat"&gt;explaining CRI-O&lt;/a&gt; during the main conference, and there&amp;rsquo;s a &lt;a href="https://kccncna17.sched.com/event/CU8j/cri-o-hosted-by-daniel-walsh-red-hat"&gt;CRI-O BOF&lt;/a&gt;.  Clayton Coleman is keynoting about &amp;ldquo;&lt;a href="https://kccncna17.sched.com/event/CUEB/keynote-red-hat-making-containers-boring-again-clayton-coleman-architect-kubernetes-and-openshift-red-hat"&gt;Making containers boring&lt;/a&gt;.&amp;rdquo;  Fabian Deutsch of KubeVirt is hosting a salon on &lt;a href="https://kccncna17.sched.com/event/CU8m/virtualizing-workloads-on-kubernetes-hosted-by-fabian-deutsch-red-hat"&gt;virtualization in Kubernetes&lt;/a&gt;.  Plus lots of other interesting sessions.&lt;/p&gt;

&lt;p&gt;Then there&amp;rsquo;s the main action, in the Red Hat Booth on the show floor.  We&amp;rsquo;ll have back-to-back demos in the mini-theater all conference long, so that you can see lots of cool technology without ever leaving the show floor.  Here&amp;rsquo;s a full schedule for you to track what&amp;rsquo;s going on; most demos will be given twice.  &lt;/p&gt;

&lt;table class="waffle" cellspacing="2" cellpadding="2"&gt;&lt;tbody&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;&lt;strong&gt;Start Time&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Topic&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Speaker&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td class="s1" dir="ltr" colspan="3"&gt;&lt;strong&gt;Wednesday&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;11:00:00 AM&lt;/td&gt;&lt;td&gt;MiniShift Hands-On&lt;/td&gt;&lt;td&gt;Adam Miller&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;11:30:00 AM&lt;/td&gt;&lt;td&gt;Live Hot Security Patching on OpenShift&lt;/td&gt;&lt;td&gt;Ryan Jarvinen&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;12:00:00 PM&lt;/td&gt;&lt;td&gt;System containers&lt;/td&gt;&lt;td&gt;Jason Brooks&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;12:30:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;01:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;01:30:00 PM&lt;/td&gt;&lt;td&gt;Container Native Storage&lt;/td&gt;&lt;td&gt;Jose Rivera&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;02:00:00 PM&lt;/td&gt;&lt;td&gt;ManageIQ-for-Kubernetes&lt;/td&gt;&lt;td&gt;Federico Simoncelli&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;02:30:00 PM&lt;/td&gt;&lt;td&gt;CI/CD Flow&lt;/td&gt;&lt;td&gt;Mauricio &amp;ldquo;Maltron&amp;rdquo; Leal&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;03:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Native Postgres&lt;/td&gt;&lt;td&gt;Josh Berkus&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;03:30:00 PM&lt;/td&gt;&lt;td&gt;OpenShift On Multi-Architecture Clusters&lt;/td&gt;&lt;td&gt;Jason DeTiberus&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;04:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift On Multi-Architecture Clusters&lt;/td&gt;&lt;td&gt;Jason DeTiberus&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;04:30:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;05:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td class="s1" dir="ltr" colspan="3"&gt;&lt;strong&gt;Wednesday Evening Booth Crawl&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;06:45:00 PM&lt;/td&gt;&lt;td&gt;New Container Tools&lt;/td&gt;&lt;td&gt;Dan Walsh&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;07:30:00 PM&lt;/td&gt;&lt;td&gt;KubeVirt&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;Fabian Deutch&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td class="s1" dir="ltr" colspan="3"&gt;&lt;strong&gt;Thursday&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;10:30:00 AM&lt;/td&gt;&lt;td&gt;CI/CD Flow&lt;/td&gt;&lt;td&gt;Mauricio &amp;ldquo;Maltron&amp;rdquo; Leal&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;11:00:00 AM&lt;/td&gt;&lt;td&gt;CRI-O&lt;/td&gt;&lt;td&gt;Mrunal Patel&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;11:30:00 AM&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;OpenShift Developer Workflow&lt;/td&gt;&lt;td&gt;Ryan Jarvinen&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;12:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;12:30:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;01:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;01:30:00 PM&lt;/td&gt;&lt;td&gt;MiniShift Hands-On&lt;/td&gt;&lt;td&gt;Adam Miller&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;02:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Native Postgres&lt;/td&gt;&lt;td&gt;Josh Berkus&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;02:30:00 PM&lt;/td&gt;&lt;td&gt;System containers&lt;/td&gt;&lt;td&gt;Jason Brooks&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;03:00:00 PM&lt;/td&gt;&lt;td&gt;KubeVirt&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;Fabian Deutch&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;03:30:00 PM&lt;/td&gt;&lt;td&gt;Container Native Storage&lt;/td&gt;&lt;td&gt;Jose Rivera&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;04:00:00 PM&lt;/td&gt;&lt;td&gt;ManageIQ-for-Kubernetes&lt;/td&gt;&lt;td&gt;Federico Simoncelli&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;04:30:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;05:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td class="s1" dir="ltr" colspan="3"&gt;&lt;strong&gt;Friday&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;10:30:00 AM&lt;/td&gt;&lt;td&gt;OpenShift On Multi-Architecture Clusters&lt;/td&gt;&lt;td&gt;Jason DeTiberus&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;11:00:00 AM&lt;/td&gt;&lt;td&gt;OpenShift On Multi-Architecture Clusters&lt;/td&gt;&lt;td&gt;Jason DeTiberus&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;11:30:00 AM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;12:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;12:30:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;01:00:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;
&lt;tr style='height:16px;'&gt;&lt;td&gt;01:30:00 PM&lt;/td&gt;&lt;td&gt;OpenShift Self-paced Lab&lt;/td&gt;&lt;td&gt;OpenShift Team&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;See you at KubeCon!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Migrating Kubernetes on Fedora Atomic Host 27</title>
    <link rel="alternate" href="http://www.projectatomic.io/blog/2017/11/migrating-kubernetes-on-fedora-atomic-host-27/"/>
    <id>http://www.projectatomic.io/blog/2017/11/migrating-kubernetes-on-fedora-atomic-host-27/</id>
    <published>2017-11-15T18:00:00+00:00</published>
    <updated>2021-10-07T14:03:16+00:00</updated>
    <author>
      <name>Jason Brooks</name>
    </author>
    <content type="html">&lt;p&gt;Starting with Fedora 27 Atomic Host, the RPMs for Kubernetes, Flannel and Etcd are no longer included in the host&amp;rsquo;s image, but are installable instead either as &lt;a href="http://www.projectatomic.io/blog/2016/09/intro-to-system-containers/"&gt;system containers&lt;/a&gt; or via &lt;a href="https://rpm-ostree.readthedocs.io/en/latest/manual/administrator-handbook/#hybrid-imagepackaging-via-package-layering"&gt;package layering&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;System containers can serve as drop-in replacements for components that had been included in the Fedora Atomic image. Once installed, these components will be manageable using the same &lt;code&gt;systemctl&lt;/code&gt; commands that apply to regular RPM-installed components. System containers are very flexible; you can easily run system container images based on CentOS, or on older (or newer) versions of Fedora on a Fedora 27 Atomic  host.&lt;/p&gt;

&lt;p&gt;Package layering makes it possible to install regular RPM packages from configured repositories. These additional &lt;q&gt;layered&lt;/q&gt; packages are persistent across upgrades, rebases, and deploys. You must typically reboot after layering on packages, and not all packages may be installed in this way. For instance, RPMs that install content to &lt;code&gt;/opt&lt;/code&gt; &lt;a href="https://github.com/projectatomic/rpm-ostree/issues/233"&gt;aren&amp;rsquo;t currently installable&lt;/a&gt; via package layering. Unlike with system containers, the packages you layer onto your host must be compatible with the version of Fedora the host is running.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re running a Kubernetes cluster on Fedora Atomic Host that depends on the baked-in versions of these components, such as a cluster installed via the Ansible scripts in the &lt;a href="https://github.com/kubernetes/contrib/tree/master/ansible"&gt;kubernetes/contrib repo&lt;/a&gt;, you&amp;rsquo;ll need to choose one of these methods to migrate your cluster when &lt;a href="http://www.projectatomic.io/blog/2017/11/fedora-atomic-26-to-27-upgrade/"&gt;upgrading to Fedora Atomic 27&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Migrating Kubernetes and related components using System Containers&lt;/h3&gt;

&lt;p&gt;To replace Kubernetes, Flannel, and Etcd with system containers, you would run the following commands. You could run these commands on a Fedora 26 Atomic Host, and then &lt;a href="http://www.projectatomic.io/blog/2017/11/fedora-atomic-26-to-27-upgrade/"&gt;upgrade to 27&lt;/a&gt;. Upon rebooting, your components and any cluster based on them should be up and running.&lt;/p&gt;

&lt;h4&gt;System containers for master nodes&lt;/h4&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# atomic install --system --system-package=no --name kube-apiserver registry.fedoraproject.org/f27/kubernetes-apiserver

# atomic install --system --system-package=no --name kube-controller-manager registry.fedoraproject.org/f27/kubernetes-controller-manager

# atomic install --system --system-package=no --name kube-scheduler registry.fedoraproject.org/f27/kubernetes-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: the &lt;code&gt;kube-apiserver&lt;/code&gt; system container provides the &lt;code&gt;kubectl&lt;/code&gt; client.&lt;/p&gt;

&lt;h4&gt;System containers for worker nodes&lt;/h4&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# atomic install --system --system-package=no --name kubelet registry.fedoraproject.org/f27/kubernetes-kubelet

# atomic install --system --system-package=no --name kube-proxy registry.fedoraproject.org/f27/kubernetes-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;System container for etcd&lt;/h4&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# atomic install --system --system-package=no --storage=ostree --name etcd registry.fedoraproject.org/f27/etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When installed with the name &lt;strong&gt;etcd&lt;/strong&gt;, the etcd system container expects to find stores etcd data in &lt;code&gt;/var/lib/etcd/etcd.etcd&lt;/code&gt;. The etcd RPM is configured by default to store data in &lt;code&gt;/var/lib/etcd/default.etcd&lt;/code&gt;, and the ansible scripts in &lt;a href="https://github.com/kubernetes/contrib/tree/master/ansible"&gt;kubernetes/contrib&lt;/a&gt; use &lt;code&gt;/var/lib/etcd&lt;/code&gt;. On a system running etcd as configured by the kubernetes/contrib ansible scripts, you&amp;rsquo;d move your data as follows:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# systemctl stop etcd

# cp -r /var/lib/etcd/member /var/lib/etcd/etcd.etcd/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: the etcd container provides the &lt;code&gt;etcdctl&lt;/code&gt; client.  There will be more about the etcd container on this blog on Friday.&lt;/p&gt;

&lt;h4&gt;System container for flannel&lt;/h4&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# atomic install --system --system-package=no --name flanneld registry.fedoraproject.org/f27/flannel
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Updating system containers&lt;/h4&gt;

&lt;p&gt;System container updates are independent of host updates. You can update a system container by pulling an updated version of the image, and then running the &lt;code&gt;atomic containers update&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# atomic pull registry.fedoraproject.org/f27/etcd
# atomic containers update etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then roll back to the previous system container version by running &lt;code&gt;atomic containers rollback&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt; # atomic containers rollback etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Migrating Kubernetes and related components using RPM Package Layering&lt;/h3&gt;

&lt;p&gt;During the &lt;a href="http://www.projectatomic.io/blog/2017/11/fedora-atomic-26-to-27-upgrade/"&gt;upgrade to 27&lt;/a&gt;, you can opt to layer on particular packages by appending &lt;code&gt;--install PACKAGE&lt;/code&gt; to the &lt;code&gt;rpm-ostree rebase&lt;/code&gt; commands. Upon rebooting into 27, your components and any cluster based on them should be up and running.&lt;/p&gt;

&lt;p&gt;To layer packages on master and etcd nodes, run the following command:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# rpm-ostree rebase fedora-atomic-27:fedora/27/x86_64/atomic-host --install kubernetes-master --install flannel --install etcd -r
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To layer packages on worker nodes, run the following command:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# rpm-ostree rebase fedora-atomic-27:fedora/27/x86_64/atomic-host --install kubernetes-node --install flannel -r
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Updating package layers&lt;/h3&gt;

&lt;p&gt;During regular rpm-ostree upgrades (with &lt;code&gt;rpm-ostree upgrade&lt;/code&gt; or &lt;code&gt;atomic host upgrade&lt;/code&gt;), your host will fetch updated package versions from your configured repositories.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Fedora 27 Atomic Released</title>
    <link rel="alternate" href="http://www.projectatomic.io/blog/2017/11/fedora-atomic-27-features/"/>
    <id>http://www.projectatomic.io/blog/2017/11/fedora-atomic-27-features/</id>
    <published>2017-11-14T16:00:00+00:00</published>
    <updated>2021-10-07T14:03:16+00:00</updated>
    <author>
      <name>Josh Berkus</name>
    </author>
    <content type="html">&lt;p&gt;&lt;a href="https://getfedora.org/en/atomic/download/"&gt;Fedora 27 Atomic Host&lt;/a&gt; is now available.  Highlights of this version include multi-architecture support, containerized Kubernetes, a single OverlayFS volume by default, and new OSTree layering capabilities.&lt;/p&gt;

&lt;p&gt;Over the next week or so, we will have additional posts on each of these features, giving technical details and use-cases.  But today, for the release, we&amp;rsquo;ll have quick summary of the major changes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3&gt;Features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi-Architecture Support&lt;/strong&gt;: Fedora 27 Atomic Host is available for &lt;a href="#"&gt;64-bit ARM&lt;/a&gt; and &lt;a href="#"&gt;Power8&lt;/a&gt; processor architectures as well as 64-bit Intel (i.e. AArch64, ppc64le and x86_64).  Not only are we distributing ISOs and cloud images for all three architectures, we will also be providing two-week OSTree updates for them as well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Containerized Kubernetes&lt;/strong&gt;: As planned, the Kubernetes binaries have been removed from the base image for Atomic Host.  This change both shrinks the base image size, and allows users to install the container orchestration platform and version of their choice, whether it&amp;rsquo;s Kubernetes, OpenShift, or something else.  Look for a blog post tommorrow on how to migrate your Kubernetes install.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Atomic Workstation Updates&lt;/strong&gt;: For over a year, Fedora contributors have been experimenting with an RPM-OStree build of Fedora Workstation, with all of their applications running in containers or Flatpaks. This build, now called &lt;q&gt;Atomic Workstation&lt;/q&gt;, will be receiving regular updates starting with this release.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One Big OverlayFS2 Volume&lt;/strong&gt;: New Atomic Host systems will now get a single filesystem volume by default, which will share binaries, system containers, and OCI/docker containers using OverlayFS2.  Users who need to partition container images and storage onto a separate volume can still do so using kickstart options and &lt;code&gt;container-storage-setup&lt;/code&gt; configuration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OSTree Package Layering Improvements&lt;/strong&gt;: RPM-OStree has added two capabilities supporting modifying individula systems: &lt;a href="/blog/2017/07/rpm-ostree-v2017.7-released/"&gt;remove and replace overrides&lt;/a&gt;, and &lt;a href="/blog/2017/06/rpm-ostree-v2017.6-released/"&gt;LiveFS layering&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With the release of Fedora 27 Atomic Host, updates to the Fedora 26 Atomic Host will be strictly on a best-effort basis.  As such, we strongly encourage users to upgrade to the new release soon. Our upgrade guide begins with &lt;a href="/blog/2017/11/fedora-atomic-26-to-27-upgrade/"&gt;this post&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Release Details&lt;/h3&gt;

&lt;p&gt;The OSTree update hash for Fedora 27 Atomic is:&lt;/p&gt;

&lt;p&gt;Version: 27.1&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Commit(x86_64): d428d3ad8ecf44e53d138042bad56a10308883a0c5d64b9c51eff27fdc9da82c&lt;/li&gt;
&lt;li&gt;Commit(aarch64): da1bd08012699a0aacaa11481d3ed617477858aab0f2ea7300168ce106202255&lt;/li&gt;
&lt;li&gt;Commit(ppc64le): 362888edfac04f8848072ae4fb8193b3da2f4fd226bef450326faff4be290abd&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are releasing images from multiple architectures but please note
that x86_64 architecture is the only one that undergoes automated
testing at this time.&lt;/p&gt;

&lt;p&gt;Existing systems can be upgraded in place via &lt;code&gt;atomic host rebase&lt;/code&gt;, &lt;code&gt;atomic host upgrade&lt;/code&gt; or
&lt;code&gt;atomic host deploy&lt;/code&gt;.  However, see the Upgrading post in this blog for &lt;a href="/blog/2017/11/fedora-atomic-26-to-27-upgrade/"&gt;more information
about upgrading from Fedora 26&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Corresponding image media for new installations can be &lt;a href="https://getfedora.org/en/atomic/download/"&gt;downloaded from Fedora&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Respective signed CHECKSUM files can be found here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/Atomic/aarch64/iso/Fedora-Atomic-27-20171110.1-aarch64-CHECKSUM"&gt;Aarch64  ISO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/Atomic/ppc64le/iso/Fedora-Atomic-27-20171110.1-ppc64le-CHECKSUM"&gt;ppc64le ISO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/Atomic/x86_64/iso/Fedora-Atomic-27-20171110.1-x86_64-CHECKSUM"&gt;x86_64 ISO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/aarch64/images/Fedora-CloudImages-27-20171110.1-aarch64-CHECKSUM"&gt;Aarch64 CloudImage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/ppc64le/images/Fedora-CloudImages-27-20171110.1-ppc64le-CHECKSUM"&gt;ppc64le CloudImage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/x86_64/images/Fedora-CloudImages-27-20171110.1-x86_64-CHECKSUM"&gt;x86_64 CloudImage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For direct download, the &lt;q&gt;latest&lt;/q&gt; targets are always available here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_iso_latest"&gt;ISO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_qcow2_latest"&gt;QCOW2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_raw_latest"&gt;Raw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_vagrant_libvirt_latest"&gt;Libvirt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_vagrant_virtualbox_latest"&gt;VirtualBox&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Filename fetching URLs are available here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_iso_latest_filename"&gt;ISO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_qcow2_latest_filename"&gt;QCOW2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_raw_latest_filename"&gt;Raw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_vagrant_libvirt_latest_filename"&gt;Libvirt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getfedora.org/atomic_vagrant_virtualbox_latest_filename"&gt;VirtualBox&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alternatively, image artifacts can be found at the following links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/Atomic/aarch64/iso/Fedora-Atomic-ostree-aarch64-27-20171110.1.iso"&gt;Fedora-Atomic-ostree-aarch64-27-20171110.1.iso&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/Atomic/ppc64le/iso/Fedora-Atomic-ostree-ppc64le-27-20171110.1.iso"&gt;Fedora-Atomic-ostree-ppc64le-27-20171110.1.iso&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/Atomic/x86_64/iso/Fedora-Atomic-ostree-x86_64-27-20171110.1.iso"&gt;Fedora-Atomic-ostree-x86_64-27-20171110.1.iso&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/aarch64/images/Fedora-Atomic-27-20171110.1.aarch64.qcow2"&gt;Fedora-Atomic-27-20171110.1.aarch64.qcow2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/aarch64/images/Fedora-Atomic-27-20171110.1.aarch64.raw.xz"&gt;Fedora-Atomic-27-20171110.1.aarch64.raw.xz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/ppc64le/images/Fedora-Atomic-27-20171110.1.ppc64le.qcow2"&gt;Fedora-Atomic-27-20171110.1.ppc64le.qcow2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/ppc64le/images/Fedora-Atomic-27-20171110.1.ppc64le.raw.xz"&gt;Fedora-Atomic-27-20171110.1.ppc64le.raw.xz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/x86_64/images/Fedora-Atomic-27-20171110.1.x86_64.qcow2"&gt;Fedora-Atomic-27-20171110.1.x86_64.qcow2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/x86_64/images/Fedora-Atomic-27-20171110.1.x86_64.raw.xz"&gt;Fedora-Atomic-27-20171110.1.x86_64.raw.xz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/x86_64/images/Fedora-Atomic-Vagrant-27-20171110.1.x86_64.vagrant-libvirt.box"&gt;Fedora-Atomic-Vagrant-27-20171110.1.x86_64.vagrant-libvirt.box&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Fedora-Atomic-27-20171110.1/CloudImages/x86_64/images/Fedora-Atomic-Vagrant-27-20171110.1.x86_64.vagrant-virtualbox.box"&gt;Fedora-Atomic-Vagrant-27-20171110.1.x86_64.vagrant-virtualbox.box&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information about the latest targets, please reference the &lt;a href="https://fedoraproject.org/wiki/Atomic_WG#Fedora_Atomic_Image_Download_Links"&gt;Fedora
Atomic Wiki space&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Do note that it can take some of the mirrors up to 12 hours to &lt;q&gt;check-in&lt;/q&gt; at
their own discretion.&lt;/p&gt;
</content>
  </entry>
</feed>
